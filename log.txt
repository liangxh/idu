# Research Log
Xihao Liang, 2016.03.17

---

## Procedure

1. 篩選出僅包含一種表情符, 文字非空, 評論非空的微博, 得出不同表情符對應的微博 mid (statica.collect_emo_mids > emo_mids.pkl)
2. 统计出不同表情符的覆蓋率 (statica.analyse_emo_mids)
3. 把emo_mids.pkl拆出子文件方便讀取 (statica.split_emo_mids > eid_mids/$EID.txt)
4. 得出不同emo 對應的mids 各自對應的uids (statica.export_uids > eid_uids/$EID.pkl)
5. 對不同eid 對應的微博篩選出4000條微博, 另外透過檢查樣例添加剔除規則 (如除去韓文在等1.中未實現) (sampler.sample(eid))
6. 觀察用戶分布情況 (userdist.py)
7. 為數據集建立字符編碼 (tfcoder.init > dataset/tfcoder.pkl)
8. 建立字符編碼組成的訓練數據 (unidatica.prepare > dataset/unidata.pkl)
8. 字符編碼-LSTM 運行 (lstm.py)
9. 在數據庫中找出數據集中每條微博的上文(該作者的前數條微博) (lastblog.get / lastblog.exists)
10. 檢查HowNet對數據集的覆蓋率, hownet對每一句的覆蓋率 (hownettest.test)
11. 未覆蓋詞檢查 (hownettestt.test)
12. 篩選出足夠多評論的微博信息 (MYSQL select user_id, mid, text, comments_count from microblogs where comments_count >= 10 limit 400000 into outfile 'blogs_400000.txt')
13. 從blogs_400000.txt 中根據blogger.extract() is not None篩選出blogs_subset.txt (commdatca.prepare)
14. 分析blogs_400000.txt (commdatica.analyse)
15. 觀察數據 (weiboparser.test_one)
	1) 評論過多者經常出現廣告
	2) 帶表情的微博一般有"質量較佳"的評論
16. 測試多線程微博評論下載 (weibolauncher.test)

17. lstm代碼修改完成 (lstm.main > data/lstm_model.npz, lstm_model.npz.pkl)
18. lstm測試結果生成 (lstm.valid > output/lstm_result.pkl)
19. lstm測試結果分析 (lstmreporter.test)

20. 實現詞表示方法
	1) 字符編碼 (id_embedder.test)
	2) 隨機詞向量 (原lstm.py中param['Wemb']) (rand_embedder.test)
	3) matrix factorization (dimreducer.svd, custom_embedder.test)
	4) denoise autocoder (dimreducer.dA, custom_embedder.test)
	5) word2vec (word2vec.py, gensimtest.py)

21. 數據字詞預處理 (datica.prepare)
22. 讀取指定已分詞數據集 (datica.load_unigram or datica.load_token)

23. blogselector思路
	1) 统计每個用戶每條微博的回覆量, 計算均值(m)與標準差(v) (blogselector.export_unmv > unmv)
	2) 篩選出均值<=50, 標準差<=100的用戶, 以排除特定明星或知名機構的微博, 其性質不明
	3) 對這些用戶提取回复量 in [5, m + v]的微博 (設定上限為m+v以盡可能排除帶垃圾回复的微博, 下限為5有利于爬蟲效率)
	4) 每人最多100條, 限制單個用戶對整個整本集的影響 (blogselector.select)
	5) 測試首條微博帶單個表情對應回复的情況　(以500+500為樣本),　發現
		i) 帶表情符的微博平均有較多回复
		ii) 帶表情符的微博的回复中帶表情符的比例較高
	#########################################################
	# mean_cc_yes:  9.728					#
	# mean_cc_no:  6.556					#
	# emo_rate_yes: 25.53%, len: 10.68, total_emo: 1242	#
	# emo_rate_no: 11.90%, len: 12.00, total_emo: 390	#
	#########################################################

	6) 其中帶表情術的微博佔24%
	7) 應考慮單個用戶發表的微博中帶表情符/不帶表情符對回复數量的影響, 排除不同同戶帶來的嘈音	

24. 5種方法對應腳本的編寫 (script_id, script_rand, script_word2vec, script_svd, script_dA)
25. jieba的詞性標注功能中'x', 'm'不單指標點符號, 導致部分分詞結果為空列表, lstm中出現bug
	* 導出x, m字符, 經過分析建立過濾字符集 (jiebatest.py)
	* x, m字符過多, 夾雜中文字符, 目前僅考慮刪去x字符, 對空列表不加入dataset (datica.load_data)

26. 樣本預處理能力未如理想, #TODO
	* 夾雜非中文字符
	* 未能透過利用jieba詞性標注對標點符號進行識別
	* 是否篩去數字

27. 鑒於期中答辯, 優先得出4 x 2種算法的分類效果
28. umtc分列為３個文件 output/umtc_subset_{0..2}.txt 分別以weibolauncher.py -i output/umtc_subset_0.txt -o blogs/raw/blogs_subset_0.txt -a 0,32 -n 6下載
29. 篩去没有評論的微博 grep -v '"commengs: \[\]"' blogs/raw/blogs_subset_0.txt blogs/bcomm/bcomm_subset_0.txt

30. 使用GPU http://deeplearning.net/software/theano/install.html#gpu-linux
31. 經dA降維后向量為全1., #TODO
32. PCA降維方法添加
33. 基於Cooc的運行與保存問題, 僅保留詞頻>=360的詞 (訓練數量360000 = 360 * 1000)
34. LSTM正確運行, 继续完成其他算法的運行, cpu平均耗時2天, 可多個進程並行
35.　gpu存儲空間不足, 為避免出現問題暫時不用
36. 爬蟲完成, 進行數據分析, 確認可用后再统一清洗
37. lstmscript作為運行抽象腳本流程,　lstmscript_xxx中的init_embedder和add_extra_options實珼對不同embedder的差异
38.　wemb_xxx實現embedder,　wordembedder作為容器, dimreducer作為统一的降維工具接口

39. 數據初步统计分析結果

	#################################################################################
	# number of blogs with emoticons: 134779 (33.69%)				#
	# number of blogs without emoticons: 265246 (66.31%)				#
	# average number of comments of blogs with emoticons: 6.65			#
	# average number of comments of blogs without emoticons: 6.32			#
	# average number of comments with emoticons of blogs with emoticons: 1.67	#
	# average number of comments with emoticons of blogs without emoticons: 1.03	#
	#################################################################################

40. 把 blogs/bcomm/bcomm_subset_0.txt結構化為 {MID, TEXT, RE} 並保存至 blogs/mtr/0.txt (python blogdata.py data/blogs/bcomm/bcomm_subset_0.txt data/blogs/mtr/0.txt # m-t-r = MID-TEXT-RE)

41. 把 mtr/IDX.txt 轉化為 {TEXT, EMO, ABOVE, FOLLOW} (python cdataextractor.py data/blogs/mtr/0.txt data/blogs/teaf.txt # t-e-a-f = TEXT-EMO-ABOVE-FOLLOW )
42. 统计分析表情符出現次數並導出到 data/blogs/emotf.pkl 和 data/blogs/eid.txt (python -c "import cdataextractor; cdataextractor.get_emotf()")
43. 把不用表情符的數據分別保存到 data/blogs/eid_data/ 下的不同文件中 (python -c "import cdataextractor; cdataextractor.split(range(90))")
44. 從 data/blogs/eid_data/篩出實驗數據到 data/blogs/dataset/raw/ (python -c "import cdataextractor; cdataextractor.prepare_dataset('dataset', 25, 4000)";　#　其中prepare_dataset可修改篩选方式, 以后以dname_dataset區分不出實驗數據集)

45. 從 data/blogs/dataset/raw/中進行 unigramize和blogger.decompose並保存到不同文件夾({text,above,follow}_unigram/)下  (python daticaext.py dataset 25)



46. 為比較不同上文間區別, 重新編寫cdataextractor.py 為cdextractor.py 重新完成數據提取
	python -c "import cdextractor; for i in range(3): cdextractor.extract('dataset', i)"
	python -c "import cdextractor; cdextractor.get_emotf('dataset')"
	python -c "import cdextractor; cdextractor.split('dataset', range(40))"
	python -c "import cdextractor; cdextractor.prepare_dataset('dataset', 25, 4000)"
	python daticaext.py dataset 25

	python -c "import contextprocessor as ctp; ctp.prepare_above_naivebayes('dataset', 'above_s_unigram', 'above_s_nb', 25)"                              
	python -c "import contextprocessor as ctp; ctp.prepare_above_naivebayes('dataset', 'above_t_unigram', 'above_t_nb', 25)"

47. 對文本上文進行測試
	sudo python lstmextscript.py -p snb2532 -x data/blogs/dataset/text_unigram/ -s data/blogs/dataset/above_s_nb/ -d 32
	sudo python lstmextscript.py -p tnb2532 -x data/blogs/dataset/text_unigram/ -s data/blogs/dataset/above_t_nb/ -d 32
	sudo python lstmextscript.py -p none2532 -x data/blogs/dataset/text_unigram/ -d 32

48. 準備表情符上文
python -c "import contextprocessor as ctp; ctp.prepare_above_emos('dataset', 'above_s_unigram', 'above_s_emo_mean', 25)"
python -c "import contextprocessor as ctp; ctp.prepare_above_emos('dataset', 'above_t_unigram', 'above_t_emo_mean', 25)"

49. 對表情符上文進行測試
sudo python lstmextscript.py -p semo2532 -x data/blogs/dataset/text_unigram/ -s data/blogs/dataset/above_s_emo_mean/ -d 32
sudo python lstmextscript.py -p temo2532 -x data/blogs/dataset/text_unigram/ -s data/blogs/dataset/above_t_emo_mean/ -d 32

50. 結合文本和上下文測試

python -c "import contextprocessor as ctp; ctp.merge('dataset', ['above_s_nb', 'above_s_emo_mean'], 'above_s_nb_emean', 25)"
python -c "import contextprocessor as ctp; ctp.merge('dataset', ['above_t_nb', 'above_t_emo_mean'], 'above_t_nb_emean', 25)"


51. 4種情感分類
	python nbscript_config.py -p nbemo4410 -k 1.0 -c data/config4.txt
	sudo python lstmscript_config.py -p emo4432 -c data/config4.txt -d 32                          

52. 對51, 懐疑數據量不足导致LSTM效果比NaiveBayes效果差, 從數據庫中重新篩選數據進行實驗

53. 重新提取數據

	python textscanner.py
	python -c "import datica; i = 0; datica.prepare_unigramize('data/dataset_emo/raw/%d.txt'%(i), 'data/dataset_emo/unigram/%d.pkl'%(i))"
	python -c "import datica; i = 1; datica.prepare_unigramize('data/dataset_emo/raw/%d.txt'%(i), 'data/dataset_emo/unigram/%d.pkl'%(i))"
	python -c "import datica; i = 2; datica.prepare_unigramize('data/dataset_emo/raw/%d.txt'%(i), 'data/dataset_emo/unigram/%d.pkl'%(i))"
	python -c "import datica; i = 3; datica.prepare_unigramize('data/dataset_emo/raw/%d.txt'%(i), 'data/dataset_emo/unigram/%d.pkl'%(i))"


	sudo python lstmscript_dir.py -p emo4708 -x data/dataset_emo/unigram/ -y 4 -d 8
	sudo python lstmscript_dir.py -p emo4732 -x data/dataset_emo/unigram/ -y 4 -d 32
	python nbscript_dir.py -p nbemo4710 -x data/dataset_emo/unigram/ -y 4 -k 1.

54. 對baseline算法Contextual knowledge, 使用cvxpy進行實驗 cvxclassifier.py data/config2x14.txt, 正負性情感各14個表情符,　最優情況準確率為70% (data/dataset/test/cvx2x14_report1.txt)

55. lstmscript_tf_config.py 對 data/config2x14.txt進行與54.比較

56. emourlexportor.extract　提取出90個表情符對應gif的url地址至 data/emodata/emourl.html
57. grapher.export_by_config　根據配置文件把多個 X_prec.pkl的準確率曲線繪畫到同一幅圖中
58. word2vec_builder.main 以數據庫中5,000,000條數據進行訓練得出詞向量, to_Wemb則把產生的模型轉換為wordembedder.WordEmbedder的數據結構

59. mismatcher.py 產生data/dataset/mismatch/,　分析命中的額外表情符與原表情符的關係

60. simtext.main 產生 simrecord_90_{0..89}.pkl 記錄測試集的相近樣本信息
61. simanalyser.main產生 ysup/%s.pkl,
	.revalidate基於已有的_test.pkl 產生multi_label結果后的_test.pkl
	.export_vote產生 data/dataset/ysup/vote_%s.pkl記錄投票結果

62. voteanalyser.py
	.count_vote產生參與投票樣本數的直方圖至data/dataset/ysup/votecount_075.png
	.vote_width產生投票樣本下滑拐點的直方圖至data/dataset/ysup/votewidth_075.png

63. regdatica.py產生regression實驗用數據

64. regression.py進行regression實驗

65. data/bow_result.txt 為以bow為輸入的實驗結果, 或 ls data/dataset/test | grep bow | grep prec.pkl


xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

記錄結果

1. 统計平均參與投票的訓練樣本數量

thr_ED	thr_rank	count
0.50	2		1.90477777778
0.50	3		2.12458333333
0.75	2		10.8983888889
0.75	3		13.0860555556

2.　sentiemo 覆蓋所有數據的比例 84.8365% (data/senticov.txt)

3. 比較各種詞向量构造方法在不同回歸模型下的效果

* 同時擬合p和a比分別擬合p和a的效果更好

* 平均誤差(歐氏距离)最小達到0.5259, 標準差為0.2735
* 對p值的擬合, 平均误差最低接近0, 標準差達到0.3
* 對a值的擬合, 平均誤差最低接近0, 但標準差值接近0.5
* 對a的誤差的標準差比p的高
* 對詞向量构造方法svdu64, 回歸模型linear_regression時, 平均誤差最小
* 對linear_regression, 按平均誤差自小至大排序為: svdu64 (0.5259) > pcau64 (0.5261) > wvu(0.5311) > wvuall(0.5259) > randu(0.5784)


## BUG

1. matplotlib.pyplot在ssh下不支持 '''no display name and no $DISPLAY environment variable'''
	* 已解決: matplotlib.use('Agg')

## NEXT

1. 數據收集
	1) 選定40,000 ~ 100,000條微博
	2) 部署到3台機器上運行
	3) 分析數據質量
	4) 決定下載更多或開始整理

## LATER

*. 修改commdatica.py作為流程腳本或測試腳本
*. 人工標注工具開發

---

Created on 2016.02.06
